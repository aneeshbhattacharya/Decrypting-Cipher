Some insights into how to approach this problem:

1. Language Modelling:
	a. Intuition is to build a model which assigns high probability to real sentences and words.
	b. Eg: "I LIKE CATS" - real sentence and should have a high probability 
	c. Eg; "Y WYRN JLOB" - unreal sentence and should have a low probability
	d. Thus, if message is decrypted correctly, model should return a high value and a low value o/w.

2. N-Grams:
	a. It is a sequence of "n" tokens.
	b. A token is generally sequence of words, but in this case we will consider them as sequence of letters.

3. Markov Model:
	a. Current state of x(t) only depends on state of x(t-1)
	b. This means:
		P(x(t)|x(t-1),x(t-2)....) = P(x(t)|x(t-1))
	c. In our case the state just depends on the letter

	Consider This Example:
		1. Consider the word "CAT"
		2. In this word if we have to analyse:
			a. P(A|C) and P(T|A).
			b. Using JUST the word "CAT" for analysis we find both probabilities = 1

		Now Larger Implementation:
			a. Suppose we now have an entire chunk of data. Like a wikipedia page or sth.
			b. Now we can analyse the probabilities of a lot of bigrams and trigrams(string of 2-3 letters respectively) Eg: AA, AB, AC etc.
			c. P(A|C) = number of times "CA" appears/ number of times C appears.

			How many bigrams do we have to calculate for 26 letters of the alphabet?
				We need to consider V^2 as 26 x 26 (2 spaces and each filled by 26 due to allowed repition)

		Practical Usage of this model:

		1. Now we see a word is made of many bigrams. 
		2. P(AB) = P(B|A).P(A)	i.e probability of the string of "AB" occuring.
		3. Now for 3 lettered word 
			P(ABC) = P(C|AB).P(B|A).P(A)

			By Markov: P(C|AB) = P(C|B)
			=> P(ABC) = P(C|B).P(B|A).P(A)

		4. Now consider any word of length T:
			P(word of length T) = P(x1).Product Of(P(X(t)|X(t-1))) taken 2 at a time.
			P(MPKLWZ) = P(M).P(Z|P).P(P|L).P(L|K).P(K|W).P(W|M)

		5. Probability of a Sentence:
			1. Since each word is separate using the mutiplication rule
				P(sentence) = Product Of (w(i)) for i in 1 to N (N is total words in the sentence).

		6. Add-one smoothing or Laplace Smoothing:
			a. Basic idea is that we saw a word one more time than it truely occured.
			b. P(x(t)|x(t-1)) = P(x(t) and x(t-1) occur)+1/ P(x(t-1) occurs)+ V
			c. V is the number of unique letters in the text

4. How to use this theoretic Model?
	Train the data on a large corpus (eg; Moby Dick).
	Then we can find the maximum likelyhood of a text being translated correctly or incorrectly.

	Practical Problem:
	Probabilities are <=1. So when we apply the mutiplication rule to words, it may be such the case that the resultant probability be so small like in the order 10^-10 that the computer
	eventually rounds to 0.
	If all probabilities are 0 we cant find the maximum probability among them.

	Solution: Use the Log-Likelyhood:
		To use this take log on both sides:
		log(P(ABC)) = log(P(C|B).P(B|A).P(A)) = log(P(C|B)) + log(P(B|A)) + log(P(A))

5. Genetic Algorithms:
	
	Basically like biological evolution. 
	To use it we make a function of DNA. 
	On each step we take the most fit values of the function and keep it.

Basic Approach to Question:

1. Make a model which dtermines if a sentence is in english or not (likelyhood function).
2. Make a DNA map / dictionary that basically maps words:
	eg: A->L
		B->Z
		c->W
		and so on...
   Using this map we are going to convert jumbled sentences to "correct" sentences.
   Calculate likelyhood function of the sentence resembling english.

3. Now, evolutionary DNA mapping:
	a. Start with a list of 20 random DNAs
	b. Map each one and get Maximum Likelyhood estimate of each.
	c. Make a list sorting the top 5 likelyhood estimates.
	d. Now in the top 5, randomize swap 2 keys in 3 separate instances.
		eg: A Z M C W K L P U -> Parent
			Z A M C W K L P U -> child1
			A Z M W C K L P U -> child2
			A Z M W K C L P U -> child3
	This is done for all 5 parents and a new list of 20 DNAs are created.
	Follow same steps above and compare likelyhood of old and new list.
	Create new list using the top 5 and repeat.
	


















 
